{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4096\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   SLength  SWidth  PLength  PWidth        class\n0      5.1     3.5      1.4     0.2  Iris-setosa\n1      4.9     3.0      1.4     0.2  Iris-setosa\n2      4.7     3.2      1.3     0.2  Iris-setosa\n3      4.6     3.1      1.5     0.2  Iris-setosa\n4      5.0     3.6      1.4     0.2  Iris-setosa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SLength</th>\n      <th>SWidth</th>\n      <th>PLength</th>\n      <th>PWidth</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "file_path = 'iris.data'\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['SLength', 'SWidth', 'PLength', 'PWidth', 'class'],\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0         Iris-setosa\n1         Iris-setosa\n2         Iris-setosa\n3         Iris-setosa\n4         Iris-setosa\n            ...      \n145    Iris-virginica\n146    Iris-virginica\n147    Iris-virginica\n148    Iris-virginica\n149    Iris-virginica\nName: class, Length: 150, dtype: category\nCategories (3, object): [Iris-setosa, Iris-versicolor, Iris-virginica]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df['class'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   SLength  SWidth  PLength  PWidth  class\n0      5.1     3.5      1.4     0.2      0\n1      4.9     3.0      1.4     0.2      0\n2      4.7     3.2      1.3     0.2      0\n3      4.6     3.1      1.5     0.2      0\n4      5.0     3.6      1.4     0.2      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SLength</th>\n      <th>SWidth</th>\n      <th>PLength</th>\n      <th>PWidth</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df['class'] = df['class'].astype('category')\n",
    "df['class'] = df['class'].cat.codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "150\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     SLength  SWidth  PLength  PWidth  class\n65       6.7     3.1      4.4     1.4      1\n80       5.5     2.4      3.8     1.1      1\n12       4.8     3.0      1.4     0.1      0\n131      7.9     3.8      6.4     2.0      2\n6        4.6     3.4      1.4     0.3      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SLength</th>\n      <th>SWidth</th>\n      <th>PLength</th>\n      <th>PWidth</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>65</th>\n      <td>6.7</td>\n      <td>3.1</td>\n      <td>4.4</td>\n      <td>1.4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>5.5</td>\n      <td>2.4</td>\n      <td>3.8</td>\n      <td>1.1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>4.8</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>7.9</td>\n      <td>3.8</td>\n      <td>6.4</td>\n      <td>2.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4.6</td>\n      <td>3.4</td>\n      <td>1.4</td>\n      <td>0.3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "n = len(df.index)\n",
    "print(n)\n",
    "shuffled_indices = np.random.permutation(n)\n",
    "df = df.iloc[shuffled_indices]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0.23796295  0.01916657  0.10870052  0.08388887]\n [-0.09537034 -0.2725      0.00700559 -0.04111111]\n [-0.28981474 -0.02250006 -0.39977404 -0.45777777]\n [ 0.57129633  0.31083325  0.44768357  0.33388886]\n [-0.34537038  0.14416665 -0.39977404 -0.37444443]]\n"
    }
   ],
   "source": [
    "x = df.iloc[:, :4].values.astype(np.float32)\n",
    "y = df.iloc[:, -1].values.astype(np.int64)\n",
    "\n",
    "mu = x.mean(axis=0)\n",
    "span = x.max(axis=0) - x.min(axis=0)\n",
    "\n",
    "def rescale(inputs):\n",
    "    return (inputs - mu) / span\n",
    "\n",
    "x = rescale(x)\n",
    "print(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(90, 4) (90,)\n(60, 4) (60,)\n"
    }
   ],
   "source": [
    "num_train = int(n * .6)\n",
    "num_test = n - num_train\n",
    "\n",
    "x_train = x[:num_train]\n",
    "y_train = y[:num_train]\n",
    "x_test = x[-num_test:]\n",
    "y_test = y[-num_test:]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        assert len(data) == len(label)\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.label = torch.from_numpy(label).long()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "90"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_dataset = NpDataset(x_train, y_train)\n",
    "test_dataset = NpDataset(x_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda\n"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "IrisNN(\n  (fn1): Linear(in_features=4, out_features=6, bias=True)\n  (fn2): Linear(in_features=6, out_features=3, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "class IrisNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNN, self).__init__()\n",
    "        \n",
    "        self.fn1 = nn.Linear(4, 6)\n",
    "        self.fn2 = nn.Linear(6, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fn1(x))\n",
    "        x = self.fn2(x)\n",
    "        return x\n",
    "    \n",
    "model = IrisNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.1867, -0.2556, -0.0844],\n        [ 0.2047, -0.2388, -0.1191],\n        [ 0.0942, -0.3886, -0.2168],\n        [ 0.2144, -0.2158, -0.0410],\n        [ 0.1083, -0.3722, -0.2131]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
    }
   ],
   "source": [
    "x, y = next(iter(train_dataloader))\n",
    "x = x[:5].to(device)\n",
    "score = model(x)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        n = x.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        score = model(x)\n",
    "        loss = loss_fn(score, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = score.max(1, keepdim=True)[1]\n",
    "        num_correct = predictions.eq(y.view_as(predictions)).sum().item()\n",
    "        \n",
    "    acc = num_correct / n\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            n = x.size(0)\n",
    "            score = model(x)\n",
    "            loss = loss_fn(score, y)\n",
    "            predictions = score.max(1, keepdim=True)[1]\n",
    "            num_correct = predictions.eq(y.view_as(predictions)).sum().item()\n",
    "        \n",
    "    acc = num_correct / n\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0/200] Train loss:1.1165 acc:33.33% - Test loss:1.1054 acc:33.33%\n[1/200] Train loss:1.1091 acc:33.33% - Test loss:1.0983 acc:33.33%\n[2/200] Train loss:1.1021 acc:33.33% - Test loss:1.0913 acc:33.33%\n[3/200] Train loss:1.0952 acc:33.33% - Test loss:1.0845 acc:33.33%\n[4/200] Train loss:1.0884 acc:33.33% - Test loss:1.0776 acc:33.33%\n[5/200] Train loss:1.0815 acc:33.33% - Test loss:1.0706 acc:33.33%\n[6/200] Train loss:1.0743 acc:33.33% - Test loss:1.0634 acc:33.33%\n[7/200] Train loss:1.0670 acc:33.33% - Test loss:1.0561 acc:33.33%\n[8/200] Train loss:1.0594 acc:33.33% - Test loss:1.0487 acc:33.33%\n[9/200] Train loss:1.0517 acc:33.33% - Test loss:1.0411 acc:33.33%\n[10/200] Train loss:1.0437 acc:33.33% - Test loss:1.0333 acc:51.67%\n[11/200] Train loss:1.0356 acc:50.00% - Test loss:1.0254 acc:58.33%\n[12/200] Train loss:1.0272 acc:55.56% - Test loss:1.0172 acc:60.00%\n[13/200] Train loss:1.0186 acc:56.67% - Test loss:1.0089 acc:63.33%\n[14/200] Train loss:1.0097 acc:58.89% - Test loss:1.0004 acc:63.33%\n[15/200] Train loss:1.0007 acc:60.00% - Test loss:0.9915 acc:65.00%\n[16/200] Train loss:0.9914 acc:63.33% - Test loss:0.9824 acc:65.00%\n[17/200] Train loss:0.9817 acc:64.44% - Test loss:0.9730 acc:65.00%\n[18/200] Train loss:0.9718 acc:65.56% - Test loss:0.9633 acc:63.33%\n[19/200] Train loss:0.9616 acc:65.56% - Test loss:0.9532 acc:65.00%\n[20/200] Train loss:0.9511 acc:65.56% - Test loss:0.9429 acc:65.00%\n[21/200] Train loss:0.9403 acc:65.56% - Test loss:0.9324 acc:65.00%\n[22/200] Train loss:0.9293 acc:66.67% - Test loss:0.9215 acc:65.00%\n[23/200] Train loss:0.9180 acc:66.67% - Test loss:0.9104 acc:65.00%\n[24/200] Train loss:0.9065 acc:65.56% - Test loss:0.8991 acc:66.67%\n[25/200] Train loss:0.8947 acc:67.78% - Test loss:0.8876 acc:70.00%\n[26/200] Train loss:0.8829 acc:67.78% - Test loss:0.8759 acc:71.67%\n[27/200] Train loss:0.8708 acc:68.89% - Test loss:0.8641 acc:73.33%\n[28/200] Train loss:0.8587 acc:72.22% - Test loss:0.8523 acc:75.00%\n[29/200] Train loss:0.8465 acc:74.44% - Test loss:0.8403 acc:80.00%\n[30/200] Train loss:0.8343 acc:74.44% - Test loss:0.8283 acc:81.67%\n[31/200] Train loss:0.8220 acc:75.56% - Test loss:0.8164 acc:83.33%\n[32/200] Train loss:0.8098 acc:77.78% - Test loss:0.8045 acc:83.33%\n[33/200] Train loss:0.7977 acc:77.78% - Test loss:0.7926 acc:83.33%\n[34/200] Train loss:0.7856 acc:77.78% - Test loss:0.7809 acc:83.33%\n[35/200] Train loss:0.7736 acc:78.89% - Test loss:0.7692 acc:83.33%\n[36/200] Train loss:0.7617 acc:80.00% - Test loss:0.7578 acc:85.00%\n[37/200] Train loss:0.7500 acc:81.11% - Test loss:0.7464 acc:85.00%\n[38/200] Train loss:0.7385 acc:81.11% - Test loss:0.7353 acc:85.00%\n[39/200] Train loss:0.7271 acc:81.11% - Test loss:0.7243 acc:88.33%\n[40/200] Train loss:0.7159 acc:82.22% - Test loss:0.7136 acc:88.33%\n[41/200] Train loss:0.7050 acc:82.22% - Test loss:0.7030 acc:88.33%\n[42/200] Train loss:0.6943 acc:81.11% - Test loss:0.6928 acc:90.00%\n[43/200] Train loss:0.6838 acc:81.11% - Test loss:0.6828 acc:90.00%\n[44/200] Train loss:0.6736 acc:80.00% - Test loss:0.6730 acc:90.00%\n[45/200] Train loss:0.6636 acc:80.00% - Test loss:0.6635 acc:90.00%\n[46/200] Train loss:0.6539 acc:82.22% - Test loss:0.6541 acc:90.00%\n[47/200] Train loss:0.6444 acc:83.33% - Test loss:0.6451 acc:90.00%\n[48/200] Train loss:0.6352 acc:85.56% - Test loss:0.6362 acc:90.00%\n[49/200] Train loss:0.6262 acc:86.67% - Test loss:0.6274 acc:90.00%\n[50/200] Train loss:0.6174 acc:86.67% - Test loss:0.6189 acc:91.67%\n[51/200] Train loss:0.6088 acc:86.67% - Test loss:0.6105 acc:91.67%\n[52/200] Train loss:0.6003 acc:88.89% - Test loss:0.6022 acc:91.67%\n[53/200] Train loss:0.5920 acc:88.89% - Test loss:0.5940 acc:93.33%\n[54/200] Train loss:0.5839 acc:88.89% - Test loss:0.5860 acc:93.33%\n[55/200] Train loss:0.5759 acc:88.89% - Test loss:0.5781 acc:93.33%\n[56/200] Train loss:0.5680 acc:90.00% - Test loss:0.5704 acc:93.33%\n[57/200] Train loss:0.5603 acc:90.00% - Test loss:0.5627 acc:93.33%\n[58/200] Train loss:0.5526 acc:90.00% - Test loss:0.5551 acc:93.33%\n[59/200] Train loss:0.5451 acc:90.00% - Test loss:0.5477 acc:93.33%\n[60/200] Train loss:0.5377 acc:90.00% - Test loss:0.5403 acc:93.33%\n[61/200] Train loss:0.5303 acc:91.11% - Test loss:0.5330 acc:93.33%\n[62/200] Train loss:0.5231 acc:92.22% - Test loss:0.5258 acc:93.33%\n[63/200] Train loss:0.5159 acc:92.22% - Test loss:0.5187 acc:93.33%\n[64/200] Train loss:0.5088 acc:92.22% - Test loss:0.5117 acc:93.33%\n[65/200] Train loss:0.5017 acc:93.33% - Test loss:0.5048 acc:93.33%\n[66/200] Train loss:0.4947 acc:92.22% - Test loss:0.4979 acc:93.33%\n[67/200] Train loss:0.4878 acc:91.11% - Test loss:0.4911 acc:93.33%\n[68/200] Train loss:0.4809 acc:91.11% - Test loss:0.4844 acc:93.33%\n[69/200] Train loss:0.4741 acc:91.11% - Test loss:0.4778 acc:93.33%\n[70/200] Train loss:0.4674 acc:91.11% - Test loss:0.4712 acc:93.33%\n[71/200] Train loss:0.4607 acc:91.11% - Test loss:0.4647 acc:95.00%\n[72/200] Train loss:0.4541 acc:91.11% - Test loss:0.4583 acc:96.67%\n[73/200] Train loss:0.4475 acc:91.11% - Test loss:0.4519 acc:96.67%\n[74/200] Train loss:0.4410 acc:91.11% - Test loss:0.4455 acc:96.67%\n[75/200] Train loss:0.4346 acc:91.11% - Test loss:0.4393 acc:96.67%\n[76/200] Train loss:0.4283 acc:91.11% - Test loss:0.4331 acc:96.67%\n[77/200] Train loss:0.4220 acc:91.11% - Test loss:0.4270 acc:96.67%\n[78/200] Train loss:0.4158 acc:91.11% - Test loss:0.4210 acc:96.67%\n[79/200] Train loss:0.4097 acc:91.11% - Test loss:0.4150 acc:96.67%\n[80/200] Train loss:0.4037 acc:91.11% - Test loss:0.4090 acc:96.67%\n[81/200] Train loss:0.3978 acc:91.11% - Test loss:0.4031 acc:96.67%\n[82/200] Train loss:0.3918 acc:91.11% - Test loss:0.3972 acc:96.67%\n[83/200] Train loss:0.3859 acc:91.11% - Test loss:0.3914 acc:96.67%\n[84/200] Train loss:0.3801 acc:92.22% - Test loss:0.3856 acc:98.33%\n[85/200] Train loss:0.3743 acc:92.22% - Test loss:0.3799 acc:98.33%\n[86/200] Train loss:0.3686 acc:93.33% - Test loss:0.3743 acc:98.33%\n[87/200] Train loss:0.3630 acc:93.33% - Test loss:0.3688 acc:98.33%\n[88/200] Train loss:0.3575 acc:93.33% - Test loss:0.3634 acc:98.33%\n[89/200] Train loss:0.3520 acc:93.33% - Test loss:0.3580 acc:98.33%\n[90/200] Train loss:0.3466 acc:94.44% - Test loss:0.3528 acc:98.33%\n[91/200] Train loss:0.3412 acc:94.44% - Test loss:0.3476 acc:98.33%\n[92/200] Train loss:0.3360 acc:94.44% - Test loss:0.3426 acc:98.33%\n[93/200] Train loss:0.3308 acc:94.44% - Test loss:0.3376 acc:98.33%\n[94/200] Train loss:0.3257 acc:94.44% - Test loss:0.3327 acc:98.33%\n[95/200] Train loss:0.3207 acc:94.44% - Test loss:0.3280 acc:98.33%\n[96/200] Train loss:0.3158 acc:94.44% - Test loss:0.3233 acc:98.33%\n[97/200] Train loss:0.3110 acc:94.44% - Test loss:0.3187 acc:98.33%\n[98/200] Train loss:0.3063 acc:94.44% - Test loss:0.3142 acc:98.33%\n[99/200] Train loss:0.3016 acc:94.44% - Test loss:0.3098 acc:98.33%\n[100/200] Train loss:0.2971 acc:94.44% - Test loss:0.3055 acc:98.33%\n[101/200] Train loss:0.2927 acc:94.44% - Test loss:0.3012 acc:98.33%\n[102/200] Train loss:0.2883 acc:94.44% - Test loss:0.2970 acc:98.33%\n[103/200] Train loss:0.2840 acc:95.56% - Test loss:0.2930 acc:98.33%\n[104/200] Train loss:0.2799 acc:95.56% - Test loss:0.2890 acc:98.33%\n[105/200] Train loss:0.2758 acc:95.56% - Test loss:0.2851 acc:98.33%\n[106/200] Train loss:0.2718 acc:95.56% - Test loss:0.2813 acc:98.33%\n[107/200] Train loss:0.2679 acc:95.56% - Test loss:0.2776 acc:98.33%\n[108/200] Train loss:0.2641 acc:95.56% - Test loss:0.2739 acc:98.33%\n[109/200] Train loss:0.2604 acc:96.67% - Test loss:0.2702 acc:96.67%\n[110/200] Train loss:0.2567 acc:96.67% - Test loss:0.2667 acc:96.67%\n[111/200] Train loss:0.2532 acc:96.67% - Test loss:0.2632 acc:96.67%\n[112/200] Train loss:0.2497 acc:96.67% - Test loss:0.2598 acc:96.67%\n[113/200] Train loss:0.2463 acc:96.67% - Test loss:0.2564 acc:96.67%\n[114/200] Train loss:0.2430 acc:96.67% - Test loss:0.2532 acc:96.67%\n[115/200] Train loss:0.2398 acc:96.67% - Test loss:0.2501 acc:95.00%\n[116/200] Train loss:0.2367 acc:96.67% - Test loss:0.2470 acc:95.00%\n[117/200] Train loss:0.2337 acc:96.67% - Test loss:0.2440 acc:95.00%\n[118/200] Train loss:0.2307 acc:96.67% - Test loss:0.2411 acc:95.00%\n[119/200] Train loss:0.2278 acc:96.67% - Test loss:0.2383 acc:95.00%\n[120/200] Train loss:0.2250 acc:96.67% - Test loss:0.2355 acc:95.00%\n[121/200] Train loss:0.2223 acc:96.67% - Test loss:0.2328 acc:95.00%\n[122/200] Train loss:0.2196 acc:96.67% - Test loss:0.2302 acc:95.00%\n[123/200] Train loss:0.2171 acc:96.67% - Test loss:0.2277 acc:95.00%\n[124/200] Train loss:0.2146 acc:96.67% - Test loss:0.2252 acc:95.00%\n[125/200] Train loss:0.2122 acc:96.67% - Test loss:0.2229 acc:95.00%\n[126/200] Train loss:0.2099 acc:96.67% - Test loss:0.2206 acc:95.00%\n[127/200] Train loss:0.2077 acc:96.67% - Test loss:0.2185 acc:95.00%\n[128/200] Train loss:0.2056 acc:96.67% - Test loss:0.2164 acc:95.00%\n[129/200] Train loss:0.2035 acc:96.67% - Test loss:0.2144 acc:95.00%\n[130/200] Train loss:0.2015 acc:96.67% - Test loss:0.2124 acc:95.00%\n[131/200] Train loss:0.1995 acc:96.67% - Test loss:0.2105 acc:95.00%\n[132/200] Train loss:0.1976 acc:96.67% - Test loss:0.2087 acc:95.00%\n[133/200] Train loss:0.1958 acc:96.67% - Test loss:0.2070 acc:95.00%\n[134/200] Train loss:0.1940 acc:96.67% - Test loss:0.2053 acc:95.00%\n[135/200] Train loss:0.1923 acc:96.67% - Test loss:0.2036 acc:95.00%\n[136/200] Train loss:0.1907 acc:96.67% - Test loss:0.2020 acc:95.00%\n[137/200] Train loss:0.1891 acc:96.67% - Test loss:0.2004 acc:95.00%\n[138/200] Train loss:0.1875 acc:96.67% - Test loss:0.1989 acc:95.00%\n[139/200] Train loss:0.1861 acc:96.67% - Test loss:0.1975 acc:95.00%\n[140/200] Train loss:0.1846 acc:97.78% - Test loss:0.1962 acc:95.00%\n[141/200] Train loss:0.1833 acc:97.78% - Test loss:0.1949 acc:95.00%\n[142/200] Train loss:0.1820 acc:97.78% - Test loss:0.1936 acc:95.00%\n[143/200] Train loss:0.1807 acc:97.78% - Test loss:0.1923 acc:95.00%\n[144/200] Train loss:0.1795 acc:97.78% - Test loss:0.1911 acc:95.00%\n[145/200] Train loss:0.1783 acc:97.78% - Test loss:0.1900 acc:95.00%\n[146/200] Train loss:0.1772 acc:97.78% - Test loss:0.1889 acc:95.00%\n[147/200] Train loss:0.1761 acc:97.78% - Test loss:0.1878 acc:95.00%\n[148/200] Train loss:0.1750 acc:97.78% - Test loss:0.1868 acc:95.00%\n[149/200] Train loss:0.1740 acc:97.78% - Test loss:0.1859 acc:95.00%\n[150/200] Train loss:0.1731 acc:97.78% - Test loss:0.1850 acc:95.00%\n[151/200] Train loss:0.1722 acc:97.78% - Test loss:0.1842 acc:95.00%\n[152/200] Train loss:0.1713 acc:97.78% - Test loss:0.1834 acc:95.00%\n[153/200] Train loss:0.1704 acc:97.78% - Test loss:0.1826 acc:95.00%\n[154/200] Train loss:0.1696 acc:97.78% - Test loss:0.1818 acc:95.00%\n[155/200] Train loss:0.1688 acc:97.78% - Test loss:0.1811 acc:95.00%\n[156/200] Train loss:0.1681 acc:97.78% - Test loss:0.1804 acc:95.00%\n[157/200] Train loss:0.1674 acc:97.78% - Test loss:0.1797 acc:95.00%\n[158/200] Train loss:0.1667 acc:97.78% - Test loss:0.1791 acc:95.00%\n[159/200] Train loss:0.1660 acc:97.78% - Test loss:0.1785 acc:95.00%\n[160/200] Train loss:0.1654 acc:97.78% - Test loss:0.1778 acc:95.00%\n[161/200] Train loss:0.1647 acc:97.78% - Test loss:0.1772 acc:95.00%\n[162/200] Train loss:0.1641 acc:97.78% - Test loss:0.1766 acc:95.00%\n[163/200] Train loss:0.1636 acc:97.78% - Test loss:0.1760 acc:95.00%\n[164/200] Train loss:0.1630 acc:97.78% - Test loss:0.1755 acc:95.00%\n[165/200] Train loss:0.1625 acc:97.78% - Test loss:0.1750 acc:95.00%\n[166/200] Train loss:0.1620 acc:97.78% - Test loss:0.1745 acc:95.00%\n[167/200] Train loss:0.1615 acc:97.78% - Test loss:0.1741 acc:95.00%\n[168/200] Train loss:0.1610 acc:97.78% - Test loss:0.1737 acc:95.00%\n[169/200] Train loss:0.1606 acc:97.78% - Test loss:0.1732 acc:95.00%\n[170/200] Train loss:0.1601 acc:97.78% - Test loss:0.1728 acc:95.00%\n[171/200] Train loss:0.1597 acc:97.78% - Test loss:0.1723 acc:95.00%\n[172/200] Train loss:0.1593 acc:97.78% - Test loss:0.1719 acc:95.00%\n[173/200] Train loss:0.1589 acc:97.78% - Test loss:0.1716 acc:95.00%\n[174/200] Train loss:0.1585 acc:97.78% - Test loss:0.1712 acc:95.00%\n[175/200] Train loss:0.1582 acc:97.78% - Test loss:0.1709 acc:95.00%\n[176/200] Train loss:0.1578 acc:97.78% - Test loss:0.1706 acc:95.00%\n[177/200] Train loss:0.1575 acc:97.78% - Test loss:0.1703 acc:95.00%\n[178/200] Train loss:0.1572 acc:97.78% - Test loss:0.1699 acc:95.00%\n[179/200] Train loss:0.1569 acc:97.78% - Test loss:0.1696 acc:95.00%\n[180/200] Train loss:0.1566 acc:97.78% - Test loss:0.1693 acc:95.00%\n[181/200] Train loss:0.1563 acc:97.78% - Test loss:0.1690 acc:95.00%\n[182/200] Train loss:0.1560 acc:97.78% - Test loss:0.1688 acc:95.00%\n[183/200] Train loss:0.1557 acc:97.78% - Test loss:0.1685 acc:95.00%\n[184/200] Train loss:0.1555 acc:97.78% - Test loss:0.1683 acc:95.00%\n[185/200] Train loss:0.1552 acc:97.78% - Test loss:0.1680 acc:95.00%\n[186/200] Train loss:0.1550 acc:97.78% - Test loss:0.1678 acc:95.00%\n[187/200] Train loss:0.1548 acc:97.78% - Test loss:0.1676 acc:95.00%\n[188/200] Train loss:0.1546 acc:97.78% - Test loss:0.1674 acc:95.00%\n[189/200] Train loss:0.1544 acc:97.78% - Test loss:0.1672 acc:95.00%\n[190/200] Train loss:0.1542 acc:97.78% - Test loss:0.1670 acc:95.00%\n[191/200] Train loss:0.1540 acc:97.78% - Test loss:0.1668 acc:95.00%\n[192/200] Train loss:0.1538 acc:97.78% - Test loss:0.1666 acc:96.67%\n[193/200] Train loss:0.1537 acc:97.78% - Test loss:0.1665 acc:96.67%\n[194/200] Train loss:0.1535 acc:97.78% - Test loss:0.1664 acc:96.67%\n[195/200] Train loss:0.1533 acc:97.78% - Test loss:0.1663 acc:96.67%\n[196/200] Train loss:0.1532 acc:97.78% - Test loss:0.1661 acc:96.67%\n[197/200] Train loss:0.1531 acc:97.78% - Test loss:0.1660 acc:96.67%\n[198/200] Train loss:0.1529 acc:97.78% - Test loss:0.1658 acc:96.67%\n[199/200] Train loss:0.1528 acc:97.78% - Test loss:0.1657 acc:96.67%\n"
    }
   ],
   "source": [
    "max_epochs = 200\n",
    "for epoch in range(max_epochs):\n",
    "    tr_loss, tr_acc = train()\n",
    "    eva_loss, eva_acc = evaluate()\n",
    "    \n",
    "    print(f'[{epoch}/{max_epochs}] Train loss:{tr_loss:.4f} acc:{tr_acc*100:.2f}% - Test loss:{eva_loss:.4f} acc:{eva_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parameter containing:\ntensor([[ 1.9710e-06, -6.9410e-06,  3.6700e-06, -2.7989e-05],\n        [ 1.3261e-01,  5.9204e-02, -1.0714e+00, -1.3477e+00],\n        [-1.2019e-01,  3.9386e-01, -1.0725e+00, -1.1610e+00],\n        [-4.9643e-01,  1.0681e+00, -1.0461e+00, -8.9905e-01],\n        [ 4.7358e-01, -9.1573e-01,  1.2762e+00,  1.3720e+00],\n        [ 1.5159e-01,  6.3947e-02, -1.1793e+00, -1.4325e+00]], device='cuda:0',\n       requires_grad=True)\nParameter containing:\ntensor([-3.9518e-04,  8.3463e-01,  7.9371e-01,  2.3454e-01,  1.3838e+00,\n         8.9896e-01], device='cuda:0', requires_grad=True)\nParameter containing:\ntensor([[ 8.4609e-06,  8.3133e-01,  8.0650e-01,  1.3783e+00, -1.0463e+00,\n          9.4610e-01],\n        [ 7.6175e-06,  7.5574e-01,  3.2095e-01, -1.0136e+00,  6.0803e-01,\n          8.7848e-01],\n        [ 1.1457e-05, -1.5385e+00, -1.2004e+00, -7.1789e-01,  1.5532e+00,\n         -1.5483e+00]], device='cuda:0', requires_grad=True)\nParameter containing:\ntensor([-0.6264,  0.1544,  0.7091], device='cuda:0', requires_grad=True)\n"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}